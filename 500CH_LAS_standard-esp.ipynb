{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from pathlib import Path\n",
    "import kaldi_io\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "from data import AudioDataLoader, AudioDataset, pad_list\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print_use = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = \"/home/meichaoyang/workspace/Listen-Attend-Spell/egs/aishell/dump/train/deltatrue/data.json\"\n",
    "test_json = \"/home/meichaoyang/workspace/Listen-Attend-Spell/egs/aishell/dump/test/deltatrue/data.json\"\n",
    "batch_size = 32\n",
    "maxlen_in = 100000\n",
    "maxlen_out = 30\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset = AudioDataset(train_json, batch_size,\n",
    "                              maxlen_in, maxlen_out)\n",
    "\n",
    "\n",
    "tr_loader = AudioDataLoader(tr_dataset, batch_size=1, num_workers=num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_dataset = AudioDataset(test_json, batch_size,\n",
    "                              maxlen_in, maxlen_out)\n",
    "te_loader = AudioDataLoader(te_dataset, batch_size=1, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_json, 'rb') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_list = []\n",
    "char_list_path = \"/home/meichaoyang/workspace/Listen-Attend-Spell/egs/aishell/data/lang_1char/train_chars.txt\"\n",
    "with open(char_list_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        data = line.split()\n",
    "        char_list.append(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH= 200\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    r\"\"\"Dot product attention.\n",
    "    Given a set of vector values, and a vector query, attention is a technique\n",
    "    to compute a weighted sum of the values, dependent on the query.\n",
    "\n",
    "    NOTE: Here we use the terminology in Stanford cs224n-2018-lecture11.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "        # TODO: move this out of this class?\n",
    "        # self.linear_out = nn.Linear(dim*2, dim)\n",
    "\n",
    "    def forward(self, queries, values):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            queries: N x To x H\n",
    "            values : N x Ti x H\n",
    "\n",
    "        Returns:\n",
    "            output: N x To x H\n",
    "            attention_distribution: N x To x Ti\n",
    "        \"\"\"\n",
    "        batch_size = queries.size(0)\n",
    "        hidden_size = queries.size(2)\n",
    "        input_lengths = values.size(1)\n",
    "        # (N, To, H) * (N, H, Ti) -> (N, To, Ti)\n",
    "        attention_scores = torch.bmm(queries, values.transpose(1, 2))\n",
    "        attention_distribution = F.softmax(\n",
    "            attention_scores.view(-1, input_lengths), dim=1).view(batch_size, -1, input_lengths)\n",
    "        # (N, To, Ti) * (N, Ti, H) -> (N, To, H)\n",
    "        attention_output = torch.bmm(attention_distribution, values)\n",
    "        # # concat -> (N, To, 2*H)\n",
    "        # concated = torch.cat((attention_output, queries), dim=2)\n",
    "        # # TODO: Move this out of this class?\n",
    "        # # output -> (N, To, H)\n",
    "        # output = torch.tanh(self.linear_out(\n",
    "        #     concated.view(-1, 2*hidden_size))).view(batch_size, -1, hidden_size)\n",
    "\n",
    "        return attention_output, attention_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    r\"\"\"Applies a multi-layer LSTM to an variable length input sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers,\n",
    "                 dropout=0.0, bidirectional=True, rnn_type='lstm'):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn_type = rnn_type\n",
    "        self.dropout = dropout\n",
    "        if self.rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                               batch_first=True,\n",
    "                               dropout=dropout,\n",
    "                               bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, padded_input, input_lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            padded_input: N x T x D\n",
    "            input_lengths: N\n",
    "\n",
    "        Returns: output, hidden\n",
    "            - **output**: N x T x H\n",
    "            - **hidden**: (num_layers * num_directions) x N x H \n",
    "        \"\"\"\n",
    "        # Add total_length for supportting nn.DataParallel() later\n",
    "        # see https://pytorch.org/docs/stable/notes/faq.html#pack-rnn-unpack-with-data-parallelism\n",
    "        total_length = padded_input.size(1)  # get the max sequence length\n",
    "        packed_input = pack_padded_sequence(padded_input, input_lengths,\n",
    "                                            batch_first=True)\n",
    "        packed_output, hidden = self.rnn(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output,\n",
    "                                        batch_first=True,\n",
    "                                        total_length=total_length)\n",
    "        return output, hidden\n",
    "\n",
    "    def flatten_parameters(self):\n",
    "        self.rnn.flatten_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, sos_id, eos_id, hidden_size,\n",
    "                 num_layers, bidirectional_encoder=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Hyper parameters\n",
    "        # embedding + output\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sos_id = sos_id  # Start of Sentence\n",
    "        self.eos_id = eos_id  # End of Sentence\n",
    "        # rnn\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional_encoder = bidirectional_encoder  # useless now\n",
    "        self.encoder_hidden_size = hidden_size  # must be equal now\n",
    "        # Components\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.rnn = nn.ModuleList()\n",
    "        self.rnn += [nn.LSTMCell(self.embedding_dim +\n",
    "                                 self.encoder_hidden_size, self.hidden_size)]\n",
    "        for l in range(1, self.num_layers):\n",
    "            self.rnn += [nn.LSTMCell(self.hidden_size, self.hidden_size)]\n",
    "        self.attention = DotProductAttention()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.encoder_hidden_size + self.hidden_size,\n",
    "                      self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_size, self.vocab_size))\n",
    "\n",
    "    def zero_state(self, encoder_padded_outputs, H=None):\n",
    "        N = encoder_padded_outputs.size(0)\n",
    "        H = self.hidden_size if H == None else H\n",
    "        return encoder_padded_outputs.new_zeros(N, H)\n",
    "\n",
    "    def forward(self, padded_input, encoder_padded_outputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            padded_input: N x To\n",
    "            # encoder_hidden: (num_layers * num_directions) x N x H\n",
    "            encoder_padded_outputs: N x Ti x H\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        # *********Get Input and Output\n",
    "        # from espnet/Decoder.forward()\n",
    "        # TODO: need to make more smart way\n",
    "        ys = [y[y != IGNORE_ID] for y in padded_input]  # parse padded ys\n",
    "        # prepare input and output word sequences with sos/eos IDs\n",
    "        eos = ys[0].new([self.eos_id])\n",
    "        sos = ys[0].new([self.sos_id])\n",
    "        ys_in = [torch.cat([sos, y], dim=0) for y in ys]\n",
    "        ys_out = [torch.cat([y, eos], dim=0) for y in ys]\n",
    "        # padding for ys with -1\n",
    "        # pys: utt x olen\n",
    "        ys_in_pad = pad_list(ys_in, self.eos_id)\n",
    "        ys_out_pad = pad_list(ys_out, IGNORE_ID)\n",
    "        # print(\"ys_in_pad\", ys_in_pad.size())\n",
    "        assert ys_in_pad.size() == ys_out_pad.size()\n",
    "        batch_size = ys_in_pad.size(0)\n",
    "        output_length = ys_in_pad.size(1)\n",
    "        # max_length = ys_in_pad.size(1) - 1  # TODO: should minus 1(sos)?\n",
    "\n",
    "        # *********Init decoder rnn\n",
    "        h_list = [self.zero_state(encoder_padded_outputs)]\n",
    "        c_list = [self.zero_state(encoder_padded_outputs)]\n",
    "        for l in range(1, self.num_layers):\n",
    "            h_list.append(self.zero_state(encoder_padded_outputs))\n",
    "            c_list.append(self.zero_state(encoder_padded_outputs))\n",
    "        att_c = self.zero_state(encoder_padded_outputs,\n",
    "                                H=encoder_padded_outputs.size(2))\n",
    "        y_all = []\n",
    "\n",
    "        # **********LAS: 1. decoder rnn 2. attention 3. concate and MLP\n",
    "        embedded = self.embedding(ys_in_pad)\n",
    "        for t in range(output_length):\n",
    "            # step 1. decoder RNN: s_i = RNN(s_i−1,y_i−1,c_i−1)\n",
    "            rnn_input = torch.cat((embedded[:, t, :], att_c), dim=1)\n",
    "            h_list[0], c_list[0] = self.rnn[0](\n",
    "                rnn_input, (h_list[0], c_list[0]))\n",
    "            for l in range(1, self.num_layers):\n",
    "                h_list[l], c_list[l] = self.rnn[l](\n",
    "                    h_list[l-1], (h_list[l], c_list[l]))\n",
    "            rnn_output = h_list[-1]  # below unsqueeze: (N x H) -> (N x 1 x H)\n",
    "            # step 2. attention: c_i = AttentionContext(s_i,h)\n",
    "            att_c, att_w = self.attention(rnn_output.unsqueeze(dim=1),\n",
    "                                          encoder_padded_outputs)\n",
    "            att_c = att_c.squeeze(dim=1)\n",
    "            # step 3. concate s_i and c_i, and input to MLP\n",
    "            mlp_input = torch.cat((rnn_output, att_c), dim=1)\n",
    "            predicted_y_t = self.mlp(mlp_input)\n",
    "            y_all.append(predicted_y_t)\n",
    "\n",
    "        y_all = torch.stack(y_all, dim=1)  # N x To x C\n",
    "        # **********Cross Entropy Loss\n",
    "        # F.cross_entropy = NLL(log_softmax(input), target))\n",
    "        y_all = y_all.view(batch_size * output_length, self.vocab_size)\n",
    "        ce_loss = F.cross_entropy(y_all, ys_out_pad.view(-1),\n",
    "                                  ignore_index=IGNORE_ID,\n",
    "                                  reduction='mean')\n",
    "\n",
    "        return ce_loss\n",
    "\n",
    "       \n",
    "\n",
    "    def recognize_beam(self, encoder_outputs, char_list, args):\n",
    "        \"\"\"Beam search, decode one utterence now.\n",
    "        Args:\n",
    "            encoder_outputs: T x H\n",
    "            char_list: list of character\n",
    "            args: args.beam\n",
    "\n",
    "        Returns:\n",
    "            nbest_hyps:\n",
    "        \"\"\"\n",
    "        # search params\n",
    "        beam = args.beam_size\n",
    "        nbest = args.nbest\n",
    "        if args.decode_max_len == 0:\n",
    "            maxlen = encoder_outputs.size(0)\n",
    "        else:\n",
    "            maxlen = args.decode_max_len\n",
    "\n",
    "        # *********Init decoder rnn\n",
    "        h_list = [self.zero_state(encoder_outputs.unsqueeze(0))]\n",
    "        c_list = [self.zero_state(encoder_outputs.unsqueeze(0))]\n",
    "        for l in range(1, self.num_layers):\n",
    "            h_list.append(self.zero_state(encoder_outputs.unsqueeze(0)))\n",
    "            c_list.append(self.zero_state(encoder_outputs.unsqueeze(0)))\n",
    "        att_c = self.zero_state(encoder_outputs.unsqueeze(0),\n",
    "                                H=encoder_outputs.unsqueeze(0).size(2))\n",
    "        # prepare sos\n",
    "        y = self.sos_id\n",
    "        vy = encoder_outputs.new_zeros(1).long()\n",
    "\n",
    "        hyp = {'score': 0.0, 'yseq': [y], 'c_prev': c_list, 'h_prev': h_list,\n",
    "               'a_prev': att_c}\n",
    "        hyps = [hyp]\n",
    "        ended_hyps = []\n",
    "\n",
    "        for i in range(maxlen):\n",
    "            hyps_best_kept = []\n",
    "            for hyp in hyps:\n",
    "                # vy.unsqueeze(1)\n",
    "                vy[0] = hyp['yseq'][i]\n",
    "                embedded = self.embedding(vy)\n",
    "                # embedded.unsqueeze(0)\n",
    "                # step 1. decoder RNN: s_i = RNN(s_i−1,y_i−1,c_i−1)\n",
    "                rnn_input = torch.cat((embedded, hyp['a_prev']), dim=1)\n",
    "                h_list[0], c_list[0] = self.rnn[0](\n",
    "                    rnn_input, (hyp['h_prev'][0], hyp['c_prev'][0]))\n",
    "                for l in range(1, self.num_layers):\n",
    "                    h_list[l], c_list[l] = self.rnn[l](\n",
    "                        h_list[l-1], (hyp['h_prev'][l], hyp['c_prev'][l]))\n",
    "                rnn_output = h_list[-1]\n",
    "                # step 2. attention: c_i = AttentionContext(s_i,h)\n",
    "                # below unsqueeze: (N x H) -> (N x 1 x H)\n",
    "                att_c, att_w = self.attention(rnn_output.unsqueeze(dim=1),\n",
    "                                              encoder_outputs.unsqueeze(0))\n",
    "                att_c = att_c.squeeze(dim=1)\n",
    "                # step 3. concate s_i and c_i, and input to MLP\n",
    "                mlp_input = torch.cat((rnn_output, att_c), dim=1)\n",
    "                predicted_y_t = self.mlp(mlp_input)\n",
    "                local_scores = F.log_softmax(predicted_y_t, dim=1)\n",
    "                # topk scores\n",
    "                local_best_scores, local_best_ids = torch.topk(\n",
    "                    local_scores, beam, dim=1)\n",
    "\n",
    "                for j in range(beam):\n",
    "                    new_hyp = {}\n",
    "                    new_hyp['h_prev'] = h_list[:]\n",
    "                    new_hyp['c_prev'] = c_list[:]\n",
    "                    new_hyp['a_prev'] = att_c[:]\n",
    "                    new_hyp['score'] = hyp['score'] + local_best_scores[0, j]\n",
    "                    new_hyp['yseq'] = [0] * (1 + len(hyp['yseq']))\n",
    "                    new_hyp['yseq'][:len(hyp['yseq'])] = hyp['yseq']\n",
    "                    new_hyp['yseq'][len(hyp['yseq'])] = int(\n",
    "                        local_best_ids[0, j])\n",
    "                    # will be (2 x beam) hyps at most\n",
    "                    hyps_best_kept.append(new_hyp)\n",
    "\n",
    "                hyps_best_kept = sorted(hyps_best_kept,\n",
    "                                        key=lambda x: x['score'],\n",
    "                                        reverse=True)[:beam]\n",
    "            # end for hyp in hyps\n",
    "            hyps = hyps_best_kept\n",
    "\n",
    "            # add eos in the final loop to avoid that there are no ended hyps\n",
    "            if i == maxlen - 1:\n",
    "                for hyp in hyps:\n",
    "                    hyp['yseq'].append(self.eos_id)\n",
    "\n",
    "            # add ended hypothes to a final list, and removed them from current hypothes\n",
    "            # (this will be a probmlem, number of hyps < beam)\n",
    "            remained_hyps = []\n",
    "            for hyp in hyps:\n",
    "                if hyp['yseq'][-1] == self.eos_id:\n",
    "                    # hyp['score'] += (i + 1) * penalty\n",
    "                    ended_hyps.append(hyp)\n",
    "                else:\n",
    "                    remained_hyps.append(hyp)\n",
    "\n",
    "            hyps = remained_hyps\n",
    "            if len(hyps) > 0:\n",
    "                print('remeined hypothes: ' + str(len(hyps)))\n",
    "            else:\n",
    "                print('no hypothesis. Finish decoding.')\n",
    "                break\n",
    "\n",
    "            for hyp in hyps:\n",
    "                print('hypo: ' + ''.join([char_list[int(x)]\n",
    "                                          for x in hyp['yseq'][1:]]))\n",
    "        # end for i in range(maxlen)\n",
    "        nbest_hyps = sorted(ended_hyps, key=lambda x: x['score'], reverse=True)[\n",
    "            :min(len(ended_hyps), nbest)]\n",
    "        return nbest_hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Sequence-to-Sequence architecture with configurable encoder and decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, padded_input, input_lengths, padded_target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            padded_input: N x Ti x D\n",
    "            padded_targets: N x To\n",
    "        \"\"\"\n",
    "        encoder_padded_outputs, _ = self.encoder(padded_input , input_lengths)\n",
    "        loss = self.decoder(padded_target, encoder_padded_outputs)\n",
    "        return loss\n",
    "    \n",
    "    def recognize(self, input, input_lengths, char_list, args):\n",
    "        \"\"\"Sequence-to-Sequence beam search, decode one utterence now.\n",
    "        Args:\n",
    "            input: T x D\n",
    "            char_list: list of characters\n",
    "            args: args.beam\n",
    "\n",
    "        Returns:\n",
    "            nbest_hyps:\n",
    "        \"\"\"\n",
    "        encoder_outputs, _ = self.encoder(input, input_lengths)\n",
    "#         print(\"encoder_outputs\", encoder_outputs.squeeze(1).shape)\n",
    "        \n",
    "        nbest_hyps = self.decoder.recognize_beam(encoder_outputs.squeeze(0), char_list, args)\n",
    "        return nbest_hyps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单步训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trainIters(model, optimizier, print_every=5, plot_every=100, learning_rate=0.01):\n",
    "#     start = time.time()\n",
    "#     n_iters = len(tr_dataset)\n",
    "#     plot_losses = []\n",
    "#     print_loss_total = 0  # Reset every print_every\n",
    "#     plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "#     encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "#     decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "# #     training_pairs = random.choices(a, k=n_iters)\n",
    "    \n",
    "#     criterion = nn.NLLLoss()\n",
    "\n",
    "# #     for utt in training_pairs:\n",
    "#     for i, (data) in enumerate(tr_loader):\n",
    "#         padded_input, input_lengths, padded_target = data\n",
    "#         padded_input, input_lengths, padded_target = data\n",
    "#         padded_input = padded_input.cuda()\n",
    "#         input_lengths = input_lengths.cuda()\n",
    "#         padded_target = padded_target.cuda()\n",
    "# #         print(\"padded_input:\",padded_input.shape)\n",
    "#         loss = model(padded_input, input_lengths, padded_target)\n",
    "# #         print(loss) #.requires_grad\n",
    "#         print_loss_total += float(loss)\n",
    "#         plot_loss_total += float(loss)\n",
    "        \n",
    "#         optimizier.zero_grad()\n",
    "#         loss.backward()\n",
    "        \n",
    "#         optimizier.step()\n",
    "\n",
    "#         if (i+1) % print_every == 0:\n",
    "#             print_loss_avg = print_loss_total / print_every\n",
    "#             print_loss_total = 0\n",
    "#             print('%s (%d %d%%) %.4f' % (timeSince(start, (i+1) / n_iters),\n",
    "#                                          (i+1), (i+1) / n_iters * 100, print_loss_avg))\n",
    "\n",
    "#         if i+1 % plot_every == 0:\n",
    "#             plot_loss_avg = plot_loss_total / plot_every\n",
    "#             plot_losses.append(plot_loss_avg)\n",
    "#             plot_loss_total = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model, epoch, optimizier, print_every=10, plot_every=10, learning_rate=0.01):\n",
    "    log = open('train_esp.log', 'w')\n",
    "    start = time.time()\n",
    "    n_iters = len(tr_dataset)\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "#     training_pairs = random.choices(a, k=n_iters)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for e in range(epoch):\n",
    "        for i, (data) in enumerate(tr_loader):\n",
    "            padded_input, input_lengths, padded_target = data\n",
    "            padded_input, input_lengths, padded_target = data\n",
    "            padded_input = padded_input.cuda()\n",
    "            input_lengths = input_lengths.cuda()\n",
    "            padded_target = padded_target.cuda()\n",
    "    #         print(\"padded_input:\",padded_input.shape)\n",
    "            loss = model(padded_input, input_lengths, padded_target)\n",
    "    #         print(loss) #.requires_grad\n",
    "            print_loss_total += float(loss)\n",
    "            plot_loss_total += float(loss)\n",
    "\n",
    "            optimizier.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizier.step()\n",
    "\n",
    "            if (i+1) % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                txt = 'Epoch %d | Iter %d | %s (%d %d%%) %.4f' % (e+1, i+1, timeSince(start, (e *n_iters +i+1) / (n_iters*epoch)),\n",
    "                                             (i+1), (e *n_iters +i+1) / (n_iters*epoch) * 100, print_loss_avg)\n",
    "                print(txt)\n",
    "                log.write(txt + \"\\n\")\n",
    "                log.flush()\n",
    "            if i+1 % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn): LSTM(240, 256, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(4233, 512)\n",
      "    (rnn): ModuleList(\n",
      "      (0): LSTMCell(1024, 512)\n",
      "    )\n",
      "    (attention): DotProductAttention()\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=512, out_features=4233, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Epoch 1 | Iter 20 | 0m 4s (- 214m 16s) (20 0%) 7.2342\n",
      "Epoch 1 | Iter 40 | 0m 7s (- 183m 53s) (40 0%) 6.7838\n",
      "Epoch 1 | Iter 60 | 0m 10s (- 168m 33s) (60 0%) 6.5869\n",
      "Epoch 1 | Iter 80 | 0m 13s (- 160m 58s) (80 0%) 6.3944\n",
      "Epoch 1 | Iter 100 | 0m 16s (- 156m 16s) (100 0%) 6.2635\n",
      "Epoch 1 | Iter 120 | 0m 19s (- 152m 23s) (120 0%) 6.1941\n",
      "Epoch 1 | Iter 140 | 0m 22s (- 149m 10s) (140 0%) 5.9539\n",
      "Epoch 1 | Iter 160 | 0m 25s (- 146m 34s) (160 0%) 5.8947\n",
      "Epoch 1 | Iter 180 | 0m 27s (- 144m 47s) (180 0%) 5.7971\n",
      "Epoch 1 | Iter 200 | 0m 30s (- 142m 38s) (200 0%) 5.7075\n",
      "Epoch 1 | Iter 220 | 0m 33s (- 140m 55s) (220 0%) 5.5040\n",
      "Epoch 1 | Iter 240 | 0m 35s (- 139m 21s) (240 0%) 5.5238\n",
      "Epoch 1 | Iter 260 | 0m 38s (- 137m 56s) (260 0%) 5.3992\n",
      "Epoch 1 | Iter 280 | 0m 40s (- 136m 43s) (280 0%) 5.3605\n"
     ]
    }
   ],
   "source": [
    "input_size = 240\n",
    "\n",
    "hidden_size = 256\n",
    "vocab_size = len(char_list)\n",
    "embedding_dim = 512\n",
    "sos_id = 0\n",
    "eos_id = 1\n",
    "learning_rate = 1e-3\n",
    "momentum = 0\n",
    "l2 = 1e-5\n",
    "\n",
    "IGNORE_ID=-1\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size, 3, dropout=0.0)\n",
    "decoder = Decoder(vocab_size, embedding_dim, sos_id, eos_id, hidden_size*2,\n",
    "                 num_layers=1, bidirectional_encoder=True)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "print(model)\n",
    "model.cuda()\n",
    "\n",
    "optimizier = torch.optim.Adam(model.parameters(),\n",
    "                                     lr=learning_rate,\n",
    "#                                      momentum=momentum,\n",
    "                                     weight_decay=l2)\n",
    "trainIters(model, 15,optimizier, print_every=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    def __init__(self, beam_size, nbest, decode_max_len):\n",
    "        self.beam_size = beam_size\n",
    "        self.nbest = nbest\n",
    "        self.decode_max_len = decode_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# te_dataset[1][1][1]#[\"input\"][0][\"feat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([800])\n",
      "input_tensor: torch.Size([1, 800, 240])\n",
      "remeined hypothes: 30\n",
      "hypo: 虽\n",
      "hypo: 最\n",
      "hypo: 罪\n",
      "hypo: 自\n",
      "hypo: 非\n",
      "hypo: 遂\n",
      "hypo: 今\n",
      "hypo: 这\n",
      "hypo: 飞\n",
      "hypo: 碎\n",
      "hypo: 嘴\n",
      "hypo: 岁\n",
      "hypo: 黑\n",
      "hypo: 昨\n",
      "hypo: 醉\n",
      "hypo: 却\n",
      "hypo: 费\n",
      "hypo: 作\n",
      "hypo: 据\n",
      "hypo: 俊\n",
      "hypo: 近\n",
      "hypo: 剧\n",
      "hypo: 瑞\n",
      "hypo: 随\n",
      "hypo: 脆\n",
      "hypo: 孙\n",
      "hypo: 尊\n",
      "hypo: 滋\n",
      "hypo: 资\n",
      "hypo: 日\n",
      "remeined hypothes: 30\n",
      "hypo: 虽然\n",
      "hypo: 最人\n",
      "hypo: 最难\n",
      "hypo: 最年\n",
      "hypo: 最终\n",
      "hypo: 最让\n",
      "hypo: 最大\n",
      "hypo: 最完\n",
      "hypo: 最近\n",
      "hypo: 自然\n",
      "hypo: 罪人\n",
      "hypo: 非常\n",
      "hypo: 最忍\n",
      "hypo: 最爱\n",
      "hypo: 最安\n",
      "hypo: 最后\n",
      "hypo: 今年\n",
      "hypo: 遂然\n",
      "hypo: 最低\n",
      "hypo: 最引\n",
      "hypo: 最严\n",
      "hypo: 最简\n",
      "hypo: 最繁\n",
      "hypo: 最便\n",
      "hypo: 最令\n",
      "hypo: 最圆\n",
      "hypo: 最远\n",
      "hypo: 最然\n",
      "hypo: 最文\n",
      "hypo: 飞人\n",
      "remeined hypothes: 30\n",
      "hypo: 虽然投\n",
      "hypo: 虽然头\n",
      "hypo: 最人投\n",
      "hypo: 虽然陶\n",
      "hypo: 最难投\n",
      "hypo: 最年投\n",
      "hypo: 虽然淘\n",
      "hypo: 最终投\n",
      "hypo: 虽然图\n",
      "hypo: 最终的\n",
      "hypo: 虽然土\n",
      "hypo: 最难头\n",
      "hypo: 虽然他\n",
      "hypo: 最让投\n",
      "hypo: 虽然妥\n",
      "hypo: 最人头\n",
      "hypo: 虽然桃\n",
      "hypo: 最完投\n",
      "hypo: 最让头\n",
      "hypo: 虽然讨\n",
      "hypo: 虽然打\n",
      "hypo: 最难的\n",
      "hypo: 虽然坦\n",
      "hypo: 虽然吐\n",
      "hypo: 最近的\n",
      "hypo: 虽然偷\n",
      "hypo: 虽然倘\n",
      "hypo: 虽然透\n",
      "hypo: 最大投\n",
      "hypo: 虽然的\n",
      "remeined hypothes: 30\n",
      "hypo: 虽然投入\n",
      "hypo: 虽然头露\n",
      "hypo: 虽然头颅\n",
      "hypo: 最人投入\n",
      "hypo: 虽然头路\n",
      "hypo: 虽然头目\n",
      "hypo: 最难投入\n",
      "hypo: 虽然陶入\n",
      "hypo: 最年投入\n",
      "hypo: 虽然陶绍\n",
      "hypo: 虽然陶露\n",
      "hypo: 虽然淘路\n",
      "hypo: 虽然投露\n",
      "hypo: 虽然头部\n",
      "hypo: 虽然头入\n",
      "hypo: 最人投露\n",
      "hypo: 虽然头乳\n",
      "hypo: 虽然头盔\n",
      "hypo: 虽然头术\n",
      "hypo: 最终投入\n",
      "hypo: 虽然头肉\n",
      "hypo: 虽然图露\n",
      "hypo: 虽然陶肉\n",
      "hypo: 虽然土路\n",
      "hypo: 虽然头怒\n",
      "hypo: 最终的投\n",
      "hypo: 虽然头雾\n",
      "hypo: 虽然头陆\n",
      "hypo: 最让投入\n",
      "hypo: 虽然头殴\n",
      "remeined hypothes: 30\n",
      "hypo: 虽然投入产\n",
      "hypo: 虽然头颅产\n",
      "hypo: 虽然头露产\n",
      "hypo: 虽然头露缠\n",
      "hypo: 最人投入产\n",
      "hypo: 最难投入产\n",
      "hypo: 虽然头路产\n",
      "hypo: 虽然头目产\n",
      "hypo: 虽然头露柴\n",
      "hypo: 虽然陶入产\n",
      "hypo: 虽然头颅缠\n",
      "hypo: 最年投入产\n",
      "hypo: 虽然头颅柴\n",
      "hypo: 虽然陶绍产\n",
      "hypo: 虽然陶露产\n",
      "hypo: 虽然淘路产\n",
      "hypo: 虽然头露燃\n",
      "hypo: 虽然投露产\n",
      "hypo: 最人投露产\n",
      "hypo: 虽然头入产\n",
      "hypo: 最终投入产\n",
      "hypo: 虽然头乳产\n",
      "hypo: 虽然头露沉\n",
      "hypo: 虽然图露产\n",
      "hypo: 虽然头露喊\n",
      "hypo: 虽然土路产\n",
      "hypo: 虽然头颅沉\n",
      "hypo: 虽然头部产\n",
      "hypo: 最终的投入\n",
      "hypo: 虽然头术产\n",
      "remeined hypothes: 30\n",
      "hypo: 虽然投入产生\n",
      "hypo: 虽然头颅产生\n",
      "hypo: 虽然头露产生\n",
      "hypo: 最人投入产生\n",
      "hypo: 虽然投入产商\n",
      "hypo: 虽然头露缠山\n",
      "hypo: 最难投入产生\n",
      "hypo: 虽然头路产生\n",
      "hypo: 虽然头目产生\n",
      "hypo: 虽然投入产业\n",
      "hypo: 虽然陶入产生\n",
      "hypo: 虽然头露柴山\n",
      "hypo: 虽然头颅缠山\n",
      "hypo: 最年投入产生\n",
      "hypo: 虽然头颅柴山\n",
      "hypo: 虽然陶绍产生\n",
      "hypo: 虽然陶露产生\n",
      "hypo: 虽然投入产山\n",
      "hypo: 虽然淘路产生\n",
      "hypo: 虽然头露燃山\n",
      "hypo: 虽然投露产生\n",
      "hypo: 虽然头露缠伤\n",
      "hypo: 最人投露产生\n",
      "hypo: 虽然头入产生\n",
      "hypo: 虽然头乳产生\n",
      "hypo: 最终投入产生\n",
      "hypo: 最人投入产商\n",
      "hypo: 虽然图露产生\n",
      "hypo: 虽然头部产生\n",
      "hypo: 虽然土路产生\n",
      "remeined hypothes: 30\n",
      "hypo: 虽然投入产生问\n",
      "hypo: 虽然头颅产生问\n",
      "hypo: 虽然头露产生问\n",
      "hypo: 最人投入产生问\n",
      "hypo: 虽然投入产商问\n",
      "hypo: 最难投入产生问\n",
      "hypo: 虽然头路产生问\n",
      "hypo: 虽然头目产生问\n",
      "hypo: 虽然陶入产生问\n",
      "hypo: 最年投入产生问\n",
      "hypo: 虽然陶绍产生问\n",
      "hypo: 虽然陶露产生问\n",
      "hypo: 虽然淘路产生问\n",
      "hypo: 虽然投露产生问\n",
      "hypo: 最人投露产生问\n",
      "hypo: 虽然头露缠伤问\n",
      "hypo: 虽然头入产生问\n",
      "hypo: 虽然头露缠山森\n",
      "hypo: 虽然头乳产生问\n",
      "hypo: 最终投入产生问\n",
      "hypo: 虽然头露缠山问\n",
      "hypo: 最人投入产商问\n",
      "hypo: 虽然头露缠山阴\n",
      "hypo: 虽然头露缠山寨\n",
      "hypo: 虽然图露产生问\n",
      "hypo: 虽然头部产生问\n",
      "hypo: 虽然投入产生万\n",
      "hypo: 虽然土路产生问\n",
      "hypo: 虽然投入产生旺\n",
      "hypo: 虽然投入产山市\n",
      "remeined hypothes: 30\n",
      "hypo: 虽然投入产生问题\n",
      "hypo: 虽然头颅产生问题\n",
      "hypo: 虽然头露产生问题\n",
      "hypo: 最人投入产生问题\n",
      "hypo: 虽然投入产商问题\n",
      "hypo: 最难投入产生问题\n",
      "hypo: 虽然头路产生问题\n",
      "hypo: 虽然头目产生问题\n",
      "hypo: 虽然陶入产生问题\n",
      "hypo: 最年投入产生问题\n",
      "hypo: 虽然陶绍产生问题\n",
      "hypo: 虽然陶露产生问题\n",
      "hypo: 虽然淘路产生问题\n",
      "hypo: 虽然投露产生问题\n",
      "hypo: 最人投露产生问题\n",
      "hypo: 虽然头露缠伤问题\n",
      "hypo: 虽然头入产生问题\n",
      "hypo: 虽然头乳产生问题\n",
      "hypo: 最终投入产生问题\n",
      "hypo: 虽然头露缠山问题\n",
      "hypo: 最人投入产商问题\n",
      "hypo: 虽然图露产生问题\n",
      "hypo: 虽然头部产生问题\n",
      "hypo: 虽然土路产生问题\n",
      "hypo: 虽然投入产生旺季\n",
      "hypo: 虽然投入产生万体\n",
      "hypo: 虽然投入产生问提\n",
      "hypo: 虽然投入产生万提\n",
      "hypo: 虽然投入产山市一\n",
      "hypo: 虽然投入产生问体\n",
      "remeined hypothes: 29\n",
      "hypo: 虽然投入产生问题继\n",
      "hypo: 虽然投入产生问题一\n",
      "hypo: 虽然投入产生问题已\n",
      "hypo: 虽然头颅产生问题继\n",
      "hypo: 虽然头颅产生问题已\n",
      "hypo: 虽然头露产生问题已\n",
      "hypo: 虽然头露产生问题继\n",
      "hypo: 虽然投入产生问题进\n",
      "hypo: 虽然投入产生问题与\n",
      "hypo: 虽然投入产商问题继\n",
      "hypo: 虽然投入产生问题借\n",
      "hypo: 虽然投入产生问题近\n",
      "hypo: 虽然投入产生问题以\n",
      "hypo: 虽然投入产生问题及\n",
      "hypo: 虽然投入产生问题既\n",
      "hypo: 虽然投入产生问题亦\n",
      "hypo: 虽然投入产生问题引\n",
      "hypo: 虽然投入产生问题也\n",
      "hypo: 最人投入产生问题继\n",
      "hypo: 最难投入产生问题继\n",
      "hypo: 虽然头路产生问题继\n",
      "hypo: 虽然投入产生问题预\n",
      "hypo: 虽然投入产生问题的\n",
      "hypo: 虽然投入产生问题自\n",
      "hypo: 虽然头露产生问题一\n",
      "hypo: 虽然头目产生问题继\n",
      "hypo: 虽然头颅产生问题一\n",
      "hypo: 虽然头目产生问题已\n",
      "hypo: 虽然投入产生问题目\n",
      "remeined hypothes: 30\n",
      "hypo: 虽然投入产生问题继续\n",
      "hypo: 虽然投入产生问题继父\n",
      "hypo: 虽然投入产生问题一起\n",
      "hypo: 虽然投入产生问题已经\n",
      "hypo: 虽然投入产生问题继妻\n",
      "hypo: 虽然投入产生问题继接\n",
      "hypo: 虽然投入产生问题继机\n",
      "hypo: 虽然头颅产生问题已经\n",
      "hypo: 虽然投入产生问题以及\n",
      "hypo: 虽然头露产生问题已经\n",
      "hypo: 虽然投入产生问题进行\n",
      "hypo: 虽然头颅产生问题继父\n",
      "hypo: 虽然投入产生问题近期\n",
      "hypo: 虽然投入产生问题引起\n",
      "hypo: 虽然投入产生问题借鉴\n",
      "hypo: 虽然投入产生问题进一\n",
      "hypo: 虽然投入产生问题继日\n",
      "hypo: 虽然头颅产生问题继续\n",
      "hypo: 虽然头露产生问题继父\n",
      "hypo: 虽然投入产生问题预期\n",
      "hypo: 虽然投入产生问题借贷\n",
      "hypo: 虽然投入产生问题已吸\n",
      "hypo: 虽然投入产商问题继续\n",
      "hypo: 虽然头露产生问题继续\n",
      "hypo: 虽然投入产生问题继纪\n",
      "hypo: 虽然投入产生问题目录\n",
      "hypo: 虽然头露产生问题一起\n",
      "hypo: 虽然头颅产生问题一起\n",
      "hypo: 虽然头颅产生问题已吸\n",
      "hypo: 虽然头露产生问题已吸\n",
      "remeined hypothes: 30\n",
      "hypo: 虽然投入产生问题继续进\n",
      "hypo: 虽然投入产生问题继父亲\n",
      "hypo: 虽然投入产生问题继接机\n",
      "hypo: 虽然投入产生问题继父母\n",
      "hypo: 虽然投入产生问题继续运\n",
      "hypo: 虽然投入产生问题已经与\n",
      "hypo: 虽然投入产生问题进行了\n",
      "hypo: 虽然投入产生问题继妻与\n",
      "hypo: 虽然投入产生问题进一步\n",
      "hypo: 虽然投入产生问题继续利\n",
      "hypo: 虽然头露产生问题已经与\n",
      "hypo: 虽然头颅产生问题已经与\n",
      "hypo: 虽然投入产生问题一起竞\n",
      "hypo: 虽然投入产生问题以及借\n",
      "hypo: 虽然投入产生问题继续继\n",
      "hypo: 虽然头颅产生问题继父亲\n",
      "hypo: 虽然投入产生问题已经有\n",
      "hypo: 虽然投入产生问题近期期\n",
      "hypo: 虽然头颅产生问题继父母\n",
      "hypo: 虽然头露产生问题继父亲\n",
      "hypo: 虽然投入产生问题继续稳\n",
      "hypo: 虽然投入产生问题继续跃\n",
      "hypo: 虽然投入产生问题已吸引\n",
      "hypo: 虽然投入产商问题继续进\n",
      "hypo: 虽然投入产生问题以及其\n",
      "hypo: 虽然投入产生问题继机进\n",
      "hypo: 虽然投入产生问题一起进\n",
      "hypo: 虽然投入产生问题继机遇\n",
      "hypo: 虽然头颅产生问题已吸引\n",
      "hypo: 虽然头露产生问题已吸引\n",
      "remeined hypothes: 30\n",
      "hypo: 虽然投入产生问题继续进行\n",
      "hypo: 虽然投入产生问题继续进一\n",
      "hypo: 虽然投入产生问题继续进入\n",
      "hypo: 虽然投入产生问题继父母亲\n",
      "hypo: 虽然投入产生问题继父亲爱\n",
      "hypo: 虽然投入产生问题继接机制\n",
      "hypo: 虽然投入产生问题一起竞争\n",
      "hypo: 虽然投入产生问题继续利益\n",
      "hypo: 虽然投入产生问题继续运行\n",
      "hypo: 虽然投入产生问题继父亲进\n",
      "hypo: 虽然头颅产生问题继父母亲\n",
      "hypo: 虽然投入产生问题继父亲自\n",
      "hypo: 虽然投入产生问题继父亲儿\n",
      "hypo: 虽然投入产生问题继续继续\n",
      "hypo: 虽然投入产生问题已吸引了\n",
      "hypo: 虽然投入产生问题继接机遇\n",
      "hypo: 虽然投入产生问题继妻与父\n",
      "hypo: 虽然投入产生问题继续运气\n",
      "hypo: 虽然投入产生问题以及借贷\n",
      "hypo: 虽然头颅产生问题已吸引了\n",
      "hypo: 虽然头露产生问题已吸引了\n",
      "hypo: 虽然投入产生问题继父亲一\n",
      "hypo: 虽然投入产生问题近期期待\n",
      "hypo: 虽然投入产生问题继续跃进\n",
      "hypo: 虽然投入产生问题继机进入\n",
      "hypo: 虽然投入产生问题继续运动\n",
      "hypo: 虽然投入产生问题继续稳提\n",
      "hypo: 虽然投入产生问题进行了目\n",
      "hypo: 虽然头颅产生问题继父亲爱\n",
      "hypo: 虽然投入产生问题以及借鉴\n",
      "remeined hypothes: 29\n",
      "hypo: 虽然投入产生问题继续进一步\n",
      "hypo: 虽然投入产生问题继续进行无\n",
      "hypo: 虽然投入产生问题继续进行了\n",
      "hypo: 虽然投入产生问题继续进行五\n",
      "hypo: 虽然投入产生问题继父亲爱立\n",
      "hypo: 虽然投入产生问题继续运行无\n",
      "hypo: 虽然投入产生问题继父母亲爱\n",
      "hypo: 虽然投入产生问题继父亲进一\n",
      "hypo: 虽然投入产生问题继续进行步\n",
      "hypo: 虽然投入产生问题继父亲儿后\n",
      "hypo: 虽然投入产生问题继续进行物\n",
      "hypo: 虽然投入产生问题继父亲自己\n",
      "hypo: 虽然投入产生问题继续进入股\n",
      "hypo: 虽然投入产生问题继续运气无\n",
      "hypo: 虽然投入产生问题继妻与父亲\n",
      "hypo: 虽然投入产生问题继续进行规\n",
      "hypo: 虽然投入产生问题继续利益去\n",
      "hypo: 虽然投入产生问题进行了目录\n",
      "hypo: 虽然投入产生问题继续进一去\n",
      "hypo: 虽然投入产生问题继续跃进入\n",
      "hypo: 虽然头颅产生问题继父母亲爱\n",
      "hypo: 虽然投入产生问题继父母亲进\n",
      "hypo: 虽然投入产生问题近期期待近\n",
      "hypo: 虽然投入产生问题继父母亲一\n",
      "hypo: 虽然投入产生问题继续稳提供\n",
      "hypo: 虽然投入产生问题继续进入乳\n",
      "hypo: 虽然投入产生问题继续进入了\n",
      "hypo: 虽然投入产生问题继续继续进\n",
      "hypo: 虽然投入产生问题继续进行目\n",
      "remeined hypothes: 28\n",
      "hypo: 虽然投入产生问题继续进一步入\n",
      "hypo: 虽然投入产生问题继续进行无误\n",
      "hypo: 虽然投入产生问题继续进行五亿\n",
      "hypo: 虽然投入产生问题继续进行了户\n",
      "hypo: 虽然投入产生问题继续运行无误\n",
      "hypo: 虽然投入产生问题继父亲进一步\n",
      "hypo: 虽然投入产生问题继续进行物目\n",
      "hypo: 虽然投入产生问题继父亲爱立即\n",
      "hypo: 虽然投入产生问题继续进行了互\n",
      "hypo: 虽然投入产生问题继父亲爱立之\n",
      "hypo: 虽然投入产生问题继续进入股票\n",
      "hypo: 虽然投入产生问题继续进行了目\n",
      "hypo: 虽然投入产生问题继续进行规划\n",
      "hypo: 虽然投入产生问题继父亲儿后企\n",
      "hypo: 虽然投入产生问题继续进一步步\n",
      "hypo: 虽然投入产生问题继续进行步目\n",
      "hypo: 虽然投入产生问题继续进行无目\n",
      "hypo: 虽然投入产生问题继续运气无误\n",
      "hypo: 虽然投入产生问题继续进行步步\n",
      "hypo: 虽然投入产生问题继父亲自己的\n",
      "hypo: 虽然投入产生问题继续继续进入\n",
      "hypo: 虽然投入产生问题继父亲儿后起\n",
      "hypo: 虽然投入产生问题继续进行无论\n",
      "hypo: 虽然投入产生问题继父亲进一去\n",
      "hypo: 虽然投入产生问题近期期待近五\n",
      "hypo: 虽然投入产生问题继续进行五月\n",
      "hypo: 虽然投入产生问题继续进行五个\n",
      "hypo: 虽然投入产生问题继父母亲进入\n",
      "remeined hypothes: 18\n",
      "hypo: 虽然投入产生问题继续进一步入市\n",
      "hypo: 虽然投入产生问题继续进行无误呢\n",
      "hypo: 虽然投入产生问题继续进行五亿户\n",
      "hypo: 虽然投入产生问题继续运行无误呢\n",
      "hypo: 虽然投入产生问题继父亲进一步入\n",
      "hypo: 虽然投入产生问题继父亲儿后企业\n",
      "hypo: 虽然投入产生问题继续进一步入去\n",
      "hypo: 虽然投入产生问题继父亲爱立即无\n",
      "hypo: 虽然投入产生问题继续进行了目录\n",
      "hypo: 虽然投入产生问题继续进一步步入\n",
      "hypo: 虽然投入产生问题近期期待近五亿\n",
      "hypo: 虽然投入产生问题继父亲爱立之无\n",
      "hypo: 虽然投入产生问题继续进一步入侵\n",
      "hypo: 虽然投入产生问题继续进行了互冷\n",
      "hypo: 虽然投入产生问题继续进行五个月\n",
      "hypo: 虽然投入产生问题继父亲自己的一\n",
      "hypo: 虽然投入产生问题继续运气无误呢\n",
      "hypo: 虽然投入产生问题继续进行五亿货\n",
      "remeined hypothes: 19\n",
      "hypo: 虽然投入产生问题继父亲进一步入市\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩\n",
      "hypo: 虽然投入产生问题继父亲爱立即无误\n",
      "hypo: 虽然投入产生问题近期期待近五亿户\n",
      "hypo: 虽然投入产生问题继父亲爱立之无误\n",
      "hypo: 虽然投入产生问题继父亲儿后企业界\n",
      "hypo: 虽然投入产生问题继父亲自己的一句\n",
      "hypo: 虽然投入产生问题继续进行五亿户人\n",
      "hypo: 虽然投入产生问题继续进行五亿户辆\n",
      "hypo: 虽然投入产生问题继父亲儿后企业务\n",
      "hypo: 虽然投入产生问题继续进行五亿户呢\n",
      "hypo: 虽然投入产生问题继续进行五亿货呢\n",
      "hypo: 虽然投入产生问题近期期待近五亿股\n",
      "hypo: 虽然投入产生问题继续进行五亿户的\n",
      "hypo: 虽然投入产生问题继父亲爱立即无论\n",
      "hypo: 虽然投入产生问题继父亲自己的一级\n",
      "hypo: 虽然投入产生问题继父亲爱立即无谓\n",
      "hypo: 虽然投入产生问题继续进行五亿户了\n",
      "hypo: 虽然投入产生问题继续进行五亿户量\n",
      "remeined hypothes: 18\n",
      "hypo: 虽然投入产生问题继父亲爱立即无误呢\n",
      "hypo: 虽然投入产生问题继父亲爱立之无误呢\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩预\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的\n",
      "hypo: 虽然投入产生问题继父亲儿后企业务的\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩如\n",
      "hypo: 虽然投入产生问题继父亲自己的一句话\n",
      "hypo: 虽然投入产生问题继父亲爱立即无误了\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩无\n",
      "hypo: 虽然投入产生问题继父亲儿后企业界瞩\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩已\n",
      "hypo: 虽然投入产生问题继父亲儿后企业界危\n",
      "hypo: 虽然投入产生问题继父亲自己的一句之\n",
      "hypo: 虽然投入产生问题继父亲爱立即无谓呢\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩与\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩领\n",
      "hypo: 虽然投入产生问题继父亲自己的一句目\n",
      "hypo: 虽然投入产生问题继父亲儿后企业界目\n",
      "remeined hypothes: 25\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩预乎\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩预计\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩预户\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩预期\n",
      "hypo: 虽然投入产生问题继父亲儿后企业界瞩目\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的回\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩领域\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩预付\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩如此\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的目\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩如无\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危\n",
      "hypo: 虽然投入产生问题继父亲自己的一句目录\n",
      "hypo: 虽然投入产生问题继父亲儿后企业界目无\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩如何\n",
      "hypo: 虽然投入产生问题继父亲自己的一句话的\n",
      "hypo: 虽然投入产生问题继父亲儿后企业界危误\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的预\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩无预\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩预会\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩预置\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的话\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩无人\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的入\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩无误\n",
      "remeined hypothes: 14\n",
      "hypo: 虽然投入产生问题继父亲儿后企业界瞩目录\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的目录\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩预计了\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的回忆\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的回复\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩预乎的\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩无预户\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的入户\n",
      "hypo: 虽然投入产生问题继父亲自己的一句目录呢\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩领域了\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的预乎\n",
      "hypo: 虽然投入产生问题继父亲儿后企业界瞩目的\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩预计的\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remeined hypothes: 16\n",
      "hypo: 虽然投入产生问题继父亲儿后企业界瞩目录呢\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的目录呢\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的入户呢\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机会\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机或\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的入户了\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机乎\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机之\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩无预户呢\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机记\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机下\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机味\n",
      "hypo: 虽然投入产生问题继父亲儿后企业界瞩目录了\n",
      "remeined hypothes: 18\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机会\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机记住\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界袭\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机下降\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机味地\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界限\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机制\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机械\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机乎让\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界面\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界度\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界惊\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界依\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机味呢\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机之间\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界也\n",
      "remeined hypothes: 17\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器人\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机械地\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界面积\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机制度\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界依然\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器换\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机会有\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界也已\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界也进\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界依依\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待遇\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待近\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机之间已\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界也不\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界度也\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待低\n",
      "remeined hypothes: 27\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器人后\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界面积极\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器人厚\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机械地进\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机械地越\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到达\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机制度起\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到极\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机制度也\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界依依然\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到几\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到近\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器换机\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界也进一\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到已\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到下\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器人口\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器人户\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到过\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到一\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到期\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到哪\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机制度达\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器人父\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机械地位\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器换到\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待近一\n",
      "remeined hypothes: 29\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器人后期\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界面积极进\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器人后起\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器人后企\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机械地进入\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器人厚齐\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到达到\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界面积极借\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机械地进一\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机界也进一步\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机械地越近\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器人厚气\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机械地进行\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器人户籍\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器换机器\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到下降\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到过去\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到已经\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器人父亲\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到近日\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到近一\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到极具\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机械地位居\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机制度达到\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器人厚及\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机械地越低\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机器人口气\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机机制度也进\n",
      "hypo: 虽然投入产生问题继父亲儿后企业绩的危机期待到哪一\n",
      "no hypothesis. Finish decoding.\n",
      "正解： 虽然投入产生问题以及机器人后期运行维护等\n"
     ]
    }
   ],
   "source": [
    "args = Args(30, 1, 25)\n",
    "b = 10\n",
    "n = 1\n",
    "sample = te_dataset[b][n][1]\n",
    "input_tensor = torch.tensor(kaldi_io.read_mat(sample[\"input\"][0][\"feat\"]))\n",
    "a = input_tensor.shape[0]//4*4\n",
    "input_tensor = input_tensor[0:(a if a < MAX_LENGTH*4 else MAX_LENGTH*4 ),:]\n",
    "\n",
    "g = input_tensor.shape[0]\n",
    "print(torch.tensor([g]))\n",
    "\n",
    "input_tensor = input_tensor.unsqueeze(0)\n",
    "print(\"input_tensor:\",input_tensor.shape)\n",
    "b = model.recognize(input_tensor.to(device),torch.tensor([g]), char_list, args)\n",
    "print(\"正解：\", sample[\"output\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.memory_cached()/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ch in data_list[1][\"corpus\"]:\n",
    "    print(ch.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.tensor([[[1,2,3],[4,5,6],[7,8,9],[10,11,12]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang.index2word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.view((4,1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}